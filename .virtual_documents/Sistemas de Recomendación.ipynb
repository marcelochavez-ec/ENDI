


# -*- coding: utf-8 -*-
"""
ETL para la ingesta de archivos de la ENDI
Versión: 1.0
Developer: Marcelo Chávez
ESPOL
Maestría en Estadística Aplicada
"""

# Borrar todas las variables en el entorno global
globals().clear()

import pyreadr
import pandas as pd
import numpy as np
from scipy.stats import kurtosis, skew
from sqlalchemy import create_engine
from tabulate import tabulate
import os

# Configurar el directorio de trabajo
os.chdir('C:/Users/marcelochavez/Documents/TESIS/')

class ETL_ENDI:
    def __init__(self,
                 db_user='postgres',
                 db_password='marce',
                 db_name='db_stat',
                 db_host='localhost', 
                 db_port='5432'):
        self.engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')
    
    def leer_rds(self, ruta_archivo):
        """Leer archivo RDS y devolver DataFrame."""
        data = pyreadr.read_r(ruta_archivo)
        return list(data.values())[0]  # Devolver primer DataFrame encontrado en el archivo
    
    def exploratorio(self, df):
        """Generar resumen estadístico de un DataFrame."""
        resumen = pd.DataFrame({
            'Variable': df.columns,
            'Tipo': df.dtypes,
            'Minimo': df.apply(lambda x: x.min() if np.issubdtype(x.dtype, np.number) else np.nan),
            'Maximo': df.apply(lambda x: x.max() if np.issubdtype(x.dtype, np.number) else np.nan),
            'Rango': df.apply(lambda x: (x.max() - x.min()) if np.issubdtype(x.dtype, np.number) else np.nan),
            'Promedio': df.apply(lambda x: round(x.mean(), 2) if np.issubdtype(x.dtype, np.number) else "No aplica"),
            'Mediana': df.apply(lambda x: round(x.median(), 2) if np.issubdtype(x.dtype, np.number) else "No aplica"),
            'Moda': df.apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),
            'Desviacion_Estandar': df.apply(lambda x: round(x.std(), 2) if np.issubdtype(x.dtype, np.number) else "No aplica"),
            'Coeficiente_Variacion': df.apply(lambda x: round(x.std() / x.mean(), 2) if np.issubdtype(x.dtype, np.number) and x.mean() != 0 else np.nan),
            'Varianza': df.apply(lambda x: round(x.var(), 2) if np.issubdtype(x.dtype, np.number) else "No aplica"),
            'Coeficiente_Asimetria': df.apply(lambda x: round(skew(x.dropna()), 2) if np.issubdtype(x.dtype, np.number) else "No aplica"),
            'Curtosis': df.apply(lambda x: round(kurtosis(x.dropna()), 2) if np.issubdtype(x.dtype, np.number) else "No aplica")
        })

        # Ajustar tipos de datos para visualización
        resumen['Tipo'] = resumen['Tipo'].replace({
            'object': 'Categórica',
            'datetime64[ns]': 'Fecha',
            'bool': 'Booleana',
            'float64': 'Numérica',
            'int64': 'Numérica'
        })
        return resumen

    def almacenamiento_postgresql(self, 
                                  df,
                                  table_name,
                                  schema='endi'):
        """Guardar DataFrame en PostgreSQL."""
        try:
            df.to_sql(table_name, 
                      self.engine, 
                      schema=schema, 
                      if_exists='replace', 
                      index=False)
            print(f"Tabla '{table_name}' almacenada en el esquema '{schema}' de la base de datos.")
        except Exception as e:
            print(f"Error al almacenar en PostgreSQL: {e}")
            
    def lectura_diccionario(self, 
                            file_path, 
                            sheets):
        """Leer múltiples pestañas de un archivo Excel y concatenar en un solo DataFrame."""
        dataframes_diccionarios = []
    
        # Leer cada pestaña, seleccionar columnas y procesar
        for sheet in sheets:
            # Leer la pestaña completa con las columnas A a F y a partir de la fila 11
            df = pd.read_excel(file_path, 
                               sheet_name=sheet, 
                               usecols='A:F', 
                               skiprows=10)  # Lee desde la fila 11, columnas A a F
    
            # Filtrar filas hasta la primera fila vacía o que contenga la palabra "Fuente" en la primera columna
            df = df[df.iloc[:, 0].notnull() &\
                    ~df.iloc[:, 0].astype(str).str.contains('Fuente:', case=False, na=False)]
    
            # Añadir el DataFrame procesado a la lista
            dataframes_diccionarios.append(df)
    
        # Concatenar todos los DataFrames en uno solo
        endi_diccionarios = pd.concat(dataframes_diccionarios, ignore_index=True)
        
        # Eliminar duplicados
        endi_diccionarios.drop_duplicates(inplace=True)
    
        # Renombrar las columnas
        endi_diccionarios.columns = ['codigo_variable', 
                                     'nombre_variable', 
                                     'pregunta', 
                                     'categorias', 
                                     'tipo_variable', 
                                     'formato']
        return endi_diccionarios
    
    def buscar_variable(self,
                        diccionario_df, 
                        variable):
        """Buscar una variable en el diccionario y devolver su contenido en formato tabular vertical."""
        # Filtrar los resultados que coinciden con el código de la variable
        resultados = diccionario_df.loc[diccionario_df['codigo_variable'] == variable]
        
        if not resultados.empty:
            # Preparar los datos para la visualización en formato vertical
            datos_verticales = []
            for idx, row in resultados.iterrows():
                for column in resultados.columns:
                    datos_verticales.append([column, row[column]])
    
            # Imprimir el resultado con tabulate, maximo dos columnas
            return tabulate(datos_verticales, headers=["Campo", "Valor"], tablefmt="fancy_grid", numalign="center")
        else:
            return f"No se encontraron resultados para la variable '{variable}'."

# Ejemplo de uso de la clase ETL_ENDI
etl_endi = ETL_ENDI()

# Path de los file de RDS
file_f1_hogar = 'C:/Users/marcelochavez/Documents/TESIS/ENDI/BDD_ENDI_R1_rds/BDD_ENDI_R1_f1_hogar.rds'
file_f1_personas = r'ENDI\BDD_ENDI_R1_rds\BDD_ENDI_R1_f1_personas.rds'
file_f2_lactancia = r'ENDI\BDD_ENDI_R1_rds\BDD_ENDI_R1_f2_lactancia.rds'
file_f2_mef = r'ENDI\BDD_ENDI_R1_rds\BDD_ENDI_R1_f2_mef.rds'
file_f2_salud_ninez = r'ENDI\BDD_ENDI_R1_rds\BDD_ENDI_R1_f2_salud_ninez.rds'

# Cargar de los archivos .RDS

f1_hogar = etl_endi.leer_rds(file_f1_hogar)
f1_personas = etl_endi.leer_rds(file_f1_personas)
f2_lactancia = etl_endi.leer_rds(file_f2_lactancia)
f2_mef = etl_endi.leer_rds(file_f2_mef)
f2_salud_niniez = etl_endi.leer_rds(file_f2_salud_ninez)

# Generar y mostrar resumen estadístico
exploratorio_hogar = etl_endi.exploratorio(f1_hogar)

# Almacenar en PostgreSQL
etl_endi.almacenamiento_postgresql(f1_hogar, 'f1_hogar')
etl_endi.almacenamiento_postgresql(f1_personas, 'f1_personas')
etl_endi.almacenamiento_postgresql(f2_lactancia, 'f2_lactancia')
etl_endi.almacenamiento_postgresql(f2_mef, 'f2_mef')
etl_endi.almacenamiento_postgresql(f2_salud_niniez, 'f2_salud_ninez')

# Ruta del archivo Excel que contiene las pestañas
file_path = 'ENDI/Diccionario_variables_ENDI_R2.xlsx'

# Lista de nombres de las pestañas que quieres leer
sheets = ['f1_personas', 
          'f1_hogar', 
          'f2_mef', 
          'f2_lactancia',
          'f2_salud_niñez']

# Llamar al método para leer las pestañas y concatenar los DataFrames
endi_diccionarios = etl_endi.lectura_diccionario(file_path, sheets)


# Búsqueda de variable en el diccionario
print(etl_endi.buscar_variable(endi_diccionarios, 'f2_s1_100_1'))























#  Importar las librerías:
import sys
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
import scipy
import sklearn as sk
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
import warnings
from great_tables import GT, exibble
import great_tables
warnings.simplefilter(action='ignore', category=FutureWarning)
print('Python:', sys.version)
print('NumPy:', np.__version__)
print('Pandas:', pd.__version__)
print('Scipy:', scipy.__version__)
print('Seaborn:', sns.__version__)
print('Matplotlib:', matplotlib.__version__)
print('Scikit-learn:', sk.__version__)
print('Great Tables:', great_tables.__version__)


#carga del conjunto de datos de calificación
ratings = pd.read_csv("https://s3-us-west-2.amazonaws.com/recommender-tutorial/ratings.csv")
ratings.head()
# cargar conjunto de datos de películas
movies = pd.read_csv("https://s3-us-west-2.amazonaws.com/recommender-tutorial/movies.csv")
movies.head()
ratings = ratings.merge(movies, how="left", on="movieId").drop_duplicates()
print("Las dimensiones del df de estudio son: ", ratings.shape)
ratings








n_ratings = len(ratings)
n_movies = len(ratings['movieId'].unique())
n_users = len(ratings['userId'].unique())
 
print(f"Número de valoraciones: {n_ratings}")
print(f"Número de movieId's: {n_movies}")
print(f"Número de usuarios únicos: {n_users}")
print(f"Promedio de Valoración por usuario: {round(n_ratings/n_users, 2)}")
print(f"Promedio de Valoración por película: {round(n_ratings/n_movies, 2)}")


from great_tables.data import gtcars
from great_tables import md, html

user_freq = ratings[['userId', 'movieId']].groupby('userId').count().reset_index()
user_freq.columns = ['userId', 'n_ratings']

# Sort the DataFrame by the number of ratings in descending order
user_freq = user_freq.sort_values("n_ratings", ascending=False).head(5)

# Create a display table showing the table tailor-made for examples: exibble
gt_tbl = user_freq[['userId', 'n_ratings']]

(
    GT(gt_tbl)
    .tab_header(title=html("<strong>Top 5 de Ratings</strong>"),
                subtitle=html("Por usuario"),)
    .tab_source_note(source_note="Fuente: Netflix")
)


query_1 = ratings[ratings.userId==414].sort_values(["rating","timestamp"], ascending=False)
query_1.head(5)


# Encuentre las películas mejor y peor valoradas:
mean_rating = ratings.groupby('movieId')[['rating']].mean()

print("============================================================================")

# Películas peor valoradas
lowest_rated = mean_rating['rating'].idxmin()
print("Película peor valorada:\n", movies.loc[movies['movieId'] == lowest_rated])

print("============================================================================")

# Películas mejor valoradas
highest_rated = mean_rating['rating'].idxmax()
print("Película mejor valorada:\n", movies.loc[movies['movieId'] == highest_rated])

print("============================================================================")





reporte_1 = ratings.groupby(["title"]).sum(["rating"]).reset_index().sort_values("rating", ascending=False).head(10)
reporte_1


def plot_horizontal_barh(df, variable_x, variable_y, title='Gráfico', xlabel=None, ylabel=None, save_path=None):
    plt.figure(figsize=(10, 6))

    bars = plt.barh(df[variable_x], df[variable_y], color='#219ebc')

    for bar in bars:
        plt.text(bar.get_width() / 2, bar.get_y() + bar.get_height() / 2,
                 f'{bar.get_width():.0f}',
                 va='center', ha='center', fontsize=10, color='black')

    plt.xlabel(xlabel if xlabel else variable_x)
    plt.ylabel(ylabel if ylabel else variable_y)
    plt.title(title)

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.tick_params(axis='x', which='both', labelsize=8)
    plt.tick_params(axis='y', which='both', labelsize=8)

    plt.figtext(0.1, 0.01, 'Elaborado por: Grupo 6', ha='left', va='bottom', fontsize=10)

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        # print(f"Figura guardada en: {save_path}")
        
    plt.show()

plot_horizontal_barh(reporte_1, 
                     'title',
                     'rating', 
                     title='Top 10 de Películas',
                     xlabel='Calificaciones o Ratings', 
                     ylabel='Películas', 
                     save_path='img/figure_2.png')


reporte_2 = ratings.groupby(["genres"]).sum(["rating"]).reset_index().sort_values("rating", ascending=False).head(10)
reporte_2


plot_horizontal_barh(reporte_2, 
                     'genres', 
                     'rating',
                     title='Top 10 de Películas por Género', 
                     xlabel='Calificaciones o Ratings',
                     ylabel='Películas', 
                     save_path='img/figure_3.png')





def create_matrix(df):
     
    N = len(df['userId'].unique())
    M = len(df['movieId'].unique())
     
    # Map Ids to indices
    user_mapper = dict(zip(np.unique(df["userId"]), list(range(N))))
    movie_mapper = dict(zip(np.unique(df["movieId"]), list(range(M))))
     
    # Map indices to IDs
    user_inv_mapper = dict(zip(list(range(N)), np.unique(df["userId"])))
    movie_inv_mapper = dict(zip(list(range(M)), np.unique(df["movieId"])))
     
    user_index = [user_mapper[i] for i in df['userId']]
    movie_index = [movie_mapper[i] for i in df['movieId']]
 
    X = csr_matrix((df["rating"], (movie_index, user_index)), shape=(M, N))
     
    return X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper
     
X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper = create_matrix(ratings)








"""
Aplicación del Método de KNN
"""
def find_similar_movies(movie_id, X, k, metric='cosine', show_distance=False):
     
    neighbour_ids = []
     
    movie_ind = movie_mapper[movie_id]
    movie_vec = X[movie_ind]
    k+=1
    kNN = NearestNeighbors(n_neighbors=k, algorithm="brute", metric=metric)
    kNN.fit(X)
    movie_vec = movie_vec.reshape(1,-1)
    neighbour = kNN.kneighbors(movie_vec, return_distance=show_distance)
    for i in range(0,k):
        n = neighbour.item(i)
        neighbour_ids.append(movie_inv_mapper[n])
    neighbour_ids.pop(0)
    return neighbour_ids
 
movie_titles = dict(zip(movies['movieId'], movies['title']))
# print(movie_titles)
movie_id = 153

similar_ids = find_similar_movies(movie_id, X, k=10)
movie_title = movie_titles[movie_id]
 
print(f"Since you watched {movie_title}")
for i in similar_ids:
    print(movie_titles[i])














def recommend_movies_for_user(user_id, X, user_mapper, movie_mapper, movie_inv_mapper, k=10):
    df1 = ratings[ratings['userId'] == user_id]
     
    if df1.empty:
        print(f"User con ID {user_id} no existe.")
        return
 
    movie_id = df1[df1['rating'] == max(df1['rating'])]['movieId'].iloc[0]
 
    movie_titles = dict(zip(movies['movieId'], movies['title']))
 
    similar_ids = find_similar_movies(movie_id, X, k)
    movie_title = movie_titles.get(movie_id, "Película no encontrada")
 
    if movie_title == "Película no encontrada":
        print(f"Película con ID {movie_id} no encontrada.")
        return
 
    print(f"Desde que viste {movie_title}, también te puede gustar:")
    for i in similar_ids:
        print(movie_titles.get(i, "Película no encontrada"))





# Vamos a filtrar a la movie_id 153 de Batman Forever y los usuarios que le dieron el mayor puntaje:
query_2 = ratings[(ratings['movieId'] == 153) & (ratings['rating'] == max(ratings['rating']))]["userId"].tolist()
type(query_2)


for user_id in query_2:
    print(f"\nRecomendaciones para el Usuario con ID {user_id}:\n")
    recommend_movies_for_user(user_id, X, user_mapper, movie_mapper, movie_inv_mapper, k=10)
    print("="*50)















